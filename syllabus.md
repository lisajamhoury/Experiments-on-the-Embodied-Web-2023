Quick links: [Session One](#session-one) | [Session Two](#session-two) | [Session Three](#session-three) 

## Course Title 

Experiments on the Embodied Web 

## Course Objectives

- Introduction to motion capture and other body sensing technology and theory 
- Introductory creative coding examples for representing the body and body movement in web frameworks. 

## Course Description

Today’s internet, made up of mostly text documents and two-dimensional images and videos, is the result of historical limitations in bandwidth, graphics processing and input devices. These limitations have made the internet a place where the mind goes, but the body cannot follow. Recent advances in motion capture devices, graphics processing, machine learning, bandwidth and browsers, however, are paving the way for the body to find its place online. Experiments on the Embodied Web will explore the new realm of embodied interactions in the browser across networks. The course will include discussion of influential works in the development of online embodied interaction, including the works of Kit Galloway and Sherrie Rabinowitz, Susan Kozel, and Laurie Anderson. Together we’ll explore pose detection across webRTC peer connections in p5.js and Three.js. Experience with Node, HTML and JavaScript is helpful but not required. ICM level programming experience is required.

## Evaluation and Policies

See [Evaluation and Policies](/policies.md).

## Guidelines

A big part of learning at ITP is learning from each other. So share your work, insights, and questions, and in exchange you'll get to see everyone else's!

## Code of Conduct

Please review the ITP/IMA [Code of Conduct](https://github.com/ITPNYU/ITP-IMA-Code-of-Conduct) and [Rules of Play](https://github.com/ITPNYU/ITP-IMA-Code-of-Conduct/blob/main/rules-of-play.md).

If you are uncomfortable with any of the class proceedings, you are always welcome to reach out to me (lisa.jamhoury@nyu.edu), our area head Mimi Yin (mimi.yin@nyu.edu), the [Bias Response Line](https://www.nyu.edu/about/policies-guidelines-compliance/equal-opportunity/bias-response/report-a-bias-incident.html), or [Title IX](https://www.nyu.edu/about/policies-guidelines-compliance/equal-opportunity/title9/reporting-and-resources/reporting-options.html).

## Time

Saturday (09/23) 12:20pm - 6:20pm 
Sunday (09/24) 12:20pm - 3:20pm 

## Mode of Instruction

This course will be offered entirely in person. If you need special accomodations please reach out. 

## Lisa's Contact

lisa.jamhoury@nyu.edu

## Support

If you find yourself struggling, remember that you have many forms of support that you can take advantage of at ITP. Look out for the [office hours and help sessions that the residents offer](https://itp.nyu.edu/residents/contact-the-residents/). 

## Content Overview

### Day One: Sunday, Sat 23, 2023

- Session One, Theory and Discussion: The Body & The Computer
- Session Two, Live Coding: The Pixel Array, Pose Detection

### Day Two : Sunday, Sept 24, 2023

- Session Three, Live Coding: Peer-to-Peer, Kinect

## Content In-Depth

## Session One

### Topic: The Body & The Computer

- How does the computer see the body? 
- History of motion capture / computer vision
- How does the body experience the computer?

### Class Discussion Materials 

- [Google Slides](https://docs.google.com/presentation/d/1vmjDEq5p21qtzI7VztvbR3D4Hpr7gPMgTlI8eGui7dg/edit?usp=sharing)
- [Closer : performance, technologies, phenomenology, by Susan Kozel pages 92-98](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/closer_2.pdf)
- [Closer audio](media/closer.m4a)
- [Plink](https://plink.in/r/ewah)
- [Group Exercise Board, requires NYU Login](https://docs.google.com/presentation/d/1TLKjTbpswpaxSs-wSzYO7K5YkVjjnk66vv29iTtHBQ0/edit?usp=sharing)

## Session Two

### Topic: Live Coding: The Pixel Array, Pose Detection

### Class Examples

 - Choose a partner for this section  
 - Examples to come
 - [Cat image](https://raw.githubusercontent.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/main/examples/assets/cat.jpg) 

### Helpful Links, Resources, Inspo
- [Coding Train 9.12: Local Server, Text Editor, JavaScript Console - p5.js Tutorial](https://www.youtube.com/watch?v=UCHzlUiDD10)
- [Coding Train 11.3: The Pixel Array - p5.js Tutorial](https://www.youtube.com/watch?v=nMUMZ5YRxHI)
- [p5 Local Server](https://github.com/processing/p5.js/wiki/Local-server)
- [npm live-server](https://www.npmjs.com/package/live-server)
- [How to Turn Your Smartphone Into a Webcam](https://www.wired.com/story/use-your-phone-as-webcam/#:~:text=Install%20EpocCam%20Webcam%20Viewer%20from,network%20and%20launch%20the%20apps.)
- [Kyle McDonald JS CV Examples](https://kylemcdonald.github.io/cv-examples/) | Some more CV examples Stick to low and medium level for week 1 assignment
- Yining Shi Examples | [Posenet / Bodypix](https://github.com/yining1023/machine-learning-for-the-web/tree/master/week3-pose) | [Handpose / Facemesh](https://github.com/yining1023/machine-learning-for-the-web/tree/master/face-hand)
- [Lingdong Huang Mediapipe Demos](https://github.com/LingDong-/handpose-facemesh-demos)
- [Pose Animator: SVG Characters with Posenet](https://blog.tensorflow.org/2020/05/pose-animator-open-source-tool-to-bring-svg-characters-to-life.html)
- [clmtrackr](https://github.com/auduno/clmtrackr) | [Live example from Kyle McDonald](https://kylemcdonald.github.io/cv-examples/FaceTracking/)
- [Posenet Documentation](https://github.com/tensorflow/tfjs-models/tree/master/posenet)
- [tfjs face landmarks documentation](https://github.com/tensorflow/tfjs-models/tree/master/face-landmarks-detection)
- [ml5js.org](https://ml5js.org/)
- Coding Train ml5.js: Pose Estimation with PoseNet: [7.1](https://thecodingtrain.com/learning/ml5/7.1-posenet.html) | [7.2](https://thecodingtrain.com/learning/ml5/7.2-pose-classifier.html) | [7.3](https://thecodingtrain.com/learning/ml5/7.3-pose-regression.html)

## Session Three 

### Topic: Live Coding: Peer-to-Peer, Kinect

### Class Examples

 - Choose a partner for this section  
 - Examples to come 

### Helpful Links, Resources, Inspo

#### General
- [Digital Dance Party by Daniel Jorssen](https://www.instagram.com/p/CASdHnMBRFt/)
- [Me and My Shadows by Ghislaine Boddington](http://www.bodydataspace.net/projects/meandmyshadow/)
- [Tips to improve your generative artwork](https://tylerxhobbs.com/essays/2018/tips-to-improve-your-generative-artwork)

#### General Kinect 
- [How to pronounce the name of Microsoft's cloud: Azure](https://www.youtube.com/watch?v=AmP11EgEM4g)
- [Kinect Windows / Azure Comparison](https://docs.microsoft.com/en-us/azure/kinect-dk/windows-comparison)
- [Azure Kinect Hardware Specification](https://docs.microsoft.com/en-us/azure/kinect-dk/hardware-specification)
- [Azure Kinect body joints documentation](https://docs.microsoft.com/en-us/azure/Kinect-dk/body-joints)
- [Understanding Kinect V2 Joints and Coordinate System](https://medium.com/@lisajamhoury/understanding-kinect-v2-joints-and-coordinate-system-4f4b90b9df16)

#### Kinectron
- [Kinectron](https://kinectron.github.io/)
- [Kinectron Server Version 0.3.5](https://github.com/kinectron/kinectron/releases/tag/0.3.5)
- [Kinectron Bootcamp Install by Jake Sherwood](https://jakesherwood.com/blog/body_ewah/kinectron-install)
- [Coding Train: Kinectron](https://www.youtube.com/watch?v=BV6xK3EOznI)
- [More Kinectron Examples](https://kinectron.github.io/docs/example-simple-skeleton.html) | Includes [Skeleton Example](https://kinectron.github.io/docs/example-skeleton-images-windows.html) 
— [Kinectron Github Repo Examples](https://github.com/kinectron/kinectron/tree/master/examples) | See [Feed Test Azure](https://github.com/kinectron/kinectron/tree/master/examples/azure_examples/p5_examples/feedTest) or [Feed Test Windows V2](https://github.com/kinectron/kinectron/tree/master/examples/windows_examples/p5_examples/feedTest) to see how all feeds work

#### Three.js 
- [Three.js Introduction](https://threejs.org/docs/#manual/en/introduction/Creating-a-scene)
- [THREE.Meshline](https://github.com/spite/THREE.MeshLine)

#### More resources
- [Kinect 2 Node Module](https://github.com/wouterverweirder/kinect2)
- [Azure Kinect Node Module](https://github.com/wouterverweirder/kinect-azure)
- [depth2web](https://github.com/js6450/depth2web)

## Further Reading 

- [Artificial Reality by Myron Krueger, pages 91-99](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/artificial_reality_5.pdf)
- [Here/There: Telepresence, Touch, and Art at the Interface (Introduction) by Kris Paulsen](https://www.academia.edu/27070858/Here_There_Telepresence_Touch_and_Art_at_the_Interface_Introduction_https_mitpress_mit_edu_books_herethere), pages 1-10 
- [Phenomenology of perception by Maurice Merleau-Ponty](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/phenomenology_of_perception_2.pdf), pages 92-94 (through "what sees and touches")
- [The End of Average, Todd Rose, pages 1-9](https://github.com/lisajamhoury/The-Body-Everywhere-and-Here-2021/blob/main/readings/end_of_avg_intro.pdf)
- ["Mismatch: How Inclusion Shapes Design" Kat Holmes](https://mitpress.mit.edu/books/mismatch)
- [Video: Design for seven billion; design for one - Kat Holmes](https://www.youtube.com/watch?v=vPH1exUrSh8)